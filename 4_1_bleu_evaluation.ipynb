{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvQkXRkN383E0hx8z+OkyF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peremartra/Apress_LLProjects_Book/blob/main/4-Evaluating%20LLMs/4_1_bleu_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div>\n",
        "    <h1>Large Language Models Projects</a></h1>\n",
        "    <h3>Apply and Implement Strategies for Large Language Models</h3>\n",
        "    <h2>4.1-BLEU,  ROUGE and N-Grams. </h2>\n",
        "    <h3>Evaluating translations with BLEU</h3>\n",
        "    <p>by <b>Pere Martra</b></p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "tcSolREOl8Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we will use the BLEU metric to compare the quality of two different approaches for performing translations.\n",
        "\n",
        "As my primary language is Spanish, I will translate a few lines from the beginning of this chapter from English to Spanish. My translations will be taken as the reference translations. In other words, they will be used as the basis upon which the quality of the automatic translations will be determined.\n",
        "\n"
      ],
      "metadata": {
        "id": "hVAkT7wBZjhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentences to Translate.\n",
        "sentences = [\n",
        "    \"In the previous chapters, you've mainly seen how to work with OpenAI models, and you've had a very practical introduction to Hugging Face's open-source models, the use of embeddings, vector databases, and agents.\",\n",
        "    \"These have been very practical chapters in which I've tried to gradually introduce concepts that have allowed you, or at least I hope so, to scale up your knowledge and start creating projects using the current technology stack of large language models.\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "BDp4f1CQp3hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spanish Translation References.\n",
        "reference_translations = [\n",
        "    [\"En los capítulos anteriores has visto mayoritariamente como trabajar con los modelos de OpenAI, y has tenido una introducción muy práctica a los modelos Open Source de Hugging Face, al uso de embeddings, las bases de datos vectoriales, los agentes.\"],\n",
        "    [\"Han sido capítulos muy prácticos en los que he intentado ir introduciendo conceptos que te han permitido, o eso espero, ir escalando en tus conocimientos y empezar a crear proyectos usando el stack tecnológico actual de los grandes modelos de lenguaje.\"]\n",
        "    ]"
      ],
      "metadata": {
        "id": "KwvAUtyGbNQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will perform the first translation using the NLLB model, a small model specialized in performing translations, which we will retrieve from Hugging Face."
      ],
      "metadata": {
        "id": "VQ3Wm2Z9gCTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "model_id = \"facebook/nllb-200-distilled-600M\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "DTO1tNuMrPn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When creating the pipeline, we pass the source language and the target language of the translation to it."
      ],
      "metadata": {
        "id": "nDlw6xChiHf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline('translation', model=model, tokenizer=tokenizer,\n",
        "                        src_lang=\"eng_Latn\", tgt_lang=\"spa_Latn\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEQT4ae9R2M5",
        "outputId": "8fbdadf1-6b60-402d-cf01-876bc32479dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translations_nllb = []\n",
        "\n",
        "for text in sentences:\n",
        "  print (\"to translate: \" + text)\n",
        "  translation = \"\"\n",
        "  translation = translator(text)\n",
        "\n",
        "  #Add the summary to summaries list\n",
        "  translations_nllb += translation[0].values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y7br7o0R3mS",
        "outputId": "a22b18b3-34fb-4d2c-a3a5-cc2cbfdd9aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to translate: In the previous chapters, you've mainly seen how to work with OpenAI models, and you've had a very practical introduction to Hugging Face's open-source models, the use of embeddings, vector databases, and agents.\n",
            "to translate: These have been very practical chapters in which I've tried to gradually introduce concepts that have allowed you, or at least I hope so, to scale up your knowledge and start creating projects using the current technology stack of large language models.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the translations stored in the list 'translations_nllb'."
      ],
      "metadata": {
        "id": "G4mfFiNSiZlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translations_nllb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGS0JZ0GWMe-",
        "outputId": "6ba03781-ca31-4ffe-922c-b25547a22488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En los capítulos anteriores, han visto principalmente cómo trabajar con modelos OpenAI, y han tenido una introducción muy práctica a los modelos de código abierto de Hugging Face, el uso de embebidos, bases de datos vectoriales y agentes.',\n",
              " 'Estos han sido capítulos muy prácticos en los que he intentado introducir gradualmente conceptos que han permitido, o al menos espero que lo hagan, ampliar sus conocimientos y comenzar a crear proyectos utilizando la tecnología actual de los modelos de lenguaje grande.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create Translations with Google Traslator.\n",
        "\n",
        "As a second source for translations, we will use the Google Translator API."
      ],
      "metadata": {
        "id": "QmtO4nZemvRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q googletrans==3.1.0a0\n",
        "from googletrans import Translator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeeOjqI9Uovs",
        "outputId": "c050b36a-1d8d-47ee-9c7f-9e393e5179b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.10/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.2.2)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2024.3.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator_google = Translator()"
      ],
      "metadata": {
        "id": "RNxy4ql9P-ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations_google = []\n",
        "\n",
        "for text in sentences:\n",
        "  print (\"to translate: \" + text)\n",
        "  translation = \"\"\n",
        "  translation = translator_google.translate(text, dest=\"es\")\n",
        "\n",
        "  #Add the summary to summaries list\n",
        "  translations_google.append(translation.text)\n",
        "  print (translation.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAJvQQ_ZQd4i",
        "outputId": "de5f202f-787f-4971-e2d9-c9cd05cb3098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "to translate: In the previous chapters, you've mainly seen how to work with OpenAI models, and you've had a very practical introduction to Hugging Face's open-source models, the use of embeddings, vector databases, and agents.\n",
            "En los capítulos anteriores, vio principalmente cómo trabajar con modelos OpenAI y tuvo una introducción muy práctica a los modelos de código abierto de Hugging Face, el uso de incrustaciones, bases de datos vectoriales y agentes.\n",
            "to translate: These have been very practical chapters in which I've tried to gradually introduce concepts that have allowed you, or at least I hope so, to scale up your knowledge and start creating projects using the current technology stack of large language models.\n",
            "Estos han sido capítulos muy prácticos en los que he intentado introducir gradualmente conceptos que te han permitido, o al menos eso espero, ampliar tus conocimientos y empezar a crear proyectos utilizando la tecnología actual de grandes modelos de lenguaje.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this list, we have the translations created by Google."
      ],
      "metadata": {
        "id": "m0M4Z5cUqC5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translations_google"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EkycPnUUL0m",
        "outputId": "61bfa17f-111d-4076-9bee-b133546d37a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En los capítulos anteriores, vio principalmente cómo trabajar con modelos OpenAI y tuvo una introducción muy práctica a los modelos de código abierto de Hugging Face, el uso de incrustaciones, bases de datos vectoriales y agentes.',\n",
              " 'Estos han sido capítulos muy prácticos en los que he intentado introducir gradualmente conceptos que te han permitido, o al menos eso espero, ampliar tus conocimientos y empezar a crear proyectos utilizando la tecnología actual de grandes modelos de lenguaje.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate translations with BLEU\n",
        "\n",
        "We will use the BLEU implementation from the Evaluate library by Hugging Face."
      ],
      "metadata": {
        "id": "jXuT5sl8ptCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q evaluate==0.4.1\n",
        "import evaluate\n",
        "bleu = evaluate.load('bleu')"
      ],
      "metadata": {
        "id": "UYNwyeY-p5Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_nllb = bleu.compute(predictions=translations_nllb, references=reference_translations)\n"
      ],
      "metadata": {
        "id": "lRAYkTNDp2qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the metrics, we pass the translated text and the reference text to the BLEU function.\n",
        "\n",
        "Note that the translated text is a list of translations:\n",
        "[\"Translation1\", \"Translation2\"]\n",
        "\n",
        "Whereas the reference texts are a list of lists of text. This allows for providing multiple references per translation:\n",
        "\n",
        "[[\"reference1 Translation1\", \"reference2 Translation1\"],\n",
        "[\"reference2 Translation2\", \"reference2 Translation2\"]]\n"
      ],
      "metadata": {
        "id": "PSkAJAfWq0SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_google = bleu.compute(predictions=translations_google, references=reference_translations)"
      ],
      "metadata": {
        "id": "qrvknChXUGPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_nllb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dop9U2ds2YKE",
        "outputId": "89225e04-1c28-4534-f367-caf373969a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.3686324165619373, 'precisions': [0.7159090909090909, 0.47674418604651164, 0.30952380952380953, 0.18292682926829268], 'brevity_penalty': 0.988700685876667, 'length_ratio': 0.9887640449438202, 'translation_length': 88, 'reference_length': 89}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_google)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "329dBCZxT0F4",
        "outputId": "f7812ae8-0a5f-4ae8-9df0-5f1f3cd7e094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'bleu': 0.44975901966417653, 'precisions': [0.7710843373493976, 0.5679012345679012, 0.4177215189873418, 0.2987012987012987], 'brevity_penalty': 0.9302618655343314, 'length_ratio': 0.9325842696629213, 'translation_length': 83, 'reference_length': 89}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the translation performed by the Google API is significantly better than the one performed by the NLLB model."
      ],
      "metadata": {
        "id": "lZgXkAq3r4mz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fvxCA0rr54W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}